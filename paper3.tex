\documentclass{article}
\usepackage{thesisstyle}
\begin{document}

\author{\and{Luke Thomas Smith} \and{Tom Horrocks} \and {Eun-Jung Holden} \and{Daniel Wedge} \and{Naveed Akhtar}}
\title{Physics-informed Implicit Neural Representation for Potential Field Geophysics}
\date{\today}
\maketitle{}

\begin{abstract}

\end{abstract}

\section{Introduction}
The advent of periodic activation functions for the well known multilayer perceptron (or fully connected network) has created a new research field, known as Implicit Neural Representation.
Implicit Neural Representation (INR) is the task of representing a signal as a function of its coordinates.
Such functions are known as coordinate multilayer perceptrons, or Coordinate MLPs.
By parameterising the function in coordinate space, novel values of the function can be queried at coordinates within the continuous spatial domain.
Periodic activation functions differ from standard linear activations, such as ReLU, by comprising the neuron with a function that repeats periodically.
Such functions include sinusoids \parencite{sitzmann2019siren}, Gaussian \parencite{ramasinghePeriodicityUnifyingFramework2022}, and most recently, wavelets \parencite{saragadamWIREWaveletImplicit2023}.

Geophysical potential fields are continuous functions in three dimensions, and are of importance to mineral exploration and geoscience.
When surveying these fields for exploration geophysics, sample locations are scattered in x, y, and z, despite best efforts to conform to a regular grid or line sample regime.
Regularising these samples to a two dimensional grid for interpretation, storage, and further machine processing is a well studied process with many methods available.
After regularisation, the grid arrays can be subject to a large number of widely used filters, such as horizontal and vertical gradient calculation and their derivatives.
These filters operate on the grid product, a representation of the potential field quantised by the cell spacing defined during the regularisation process.

If a potential field survey could instead be encoded within an Implicit Neural Network as an INR, filters could be calculated analytically in the continuous three dimensional domain.
With this representation the extraction of regularised two dimensional slices, namely grids, could be performed \emph{a posteri} of gridding.
This would allow the use of all available original sample data in calculation of filters, and reduce the manual intervention required during the gridding process.
Additionally, due to the continuous representation, grid resolution is not constrained by cell size selection.




\subsection{Implicit Neural Representation}
\label{sec:inr}
Seminal work by \parencite{mildenhallNeRFRepresentingScenes2020} demonstrated the capacity for learning continuous functions from a set of coordinates.
However, it was not until they were combined with periodic activation functions \parencite{sitzmann2019siren} that the implicit representations could adequately recreate high frequency signal.
Following investigation by \parencite{ramasinghePeriodicityUnifyingFramework2022} shows that it is the magnitude of the first and second derivatives that enhance high-frequency capacity, rather than periodicity.

Realising their power in implicit representation for natural images, many works quicky investigated
\begin{enumerate}
    \item MLPs
    \item Periodic activations
    \item Storage weights not cells
\end{enumerate}

\subsection{Automatic Differentiation}
Automatic differentiation (AD) refers to one of several methods for computing derivatives, specifically to the accumulation of derivatives during function evaluation \parencite{baydin2018automatic}.
This method underpins modern machine learning frameworks such as Pytorch \parencite{paszkePyTorchImperativeStyle2019}.
In these frameworks, when evaluating a set of inputs against the models weight parameters, the derivatives with respect to the parameters are used to perform backpropogation when evaluating an objective function (loss).
These derivative inform weight updates in the direction of the negative gradient, toward a minima in the objective function (for example, L2 norm between predictions and targets).
However, automatic differentiation can also evaluate the derivatives with respect to the inputs.
The gradient provides partial derivatives $\frac{\partial f}{\partial x}$, $\frac{\partial f}{\partial y}$, $\frac{\partial f}{\partial z}$.

\subsection{Physics-Informed Neural Networks}
\parencite{raissiPhysicsinformedNeuralNetworks2019} introduced Physics-informed neural networks (PINNs).
When dealing with samples of phenomena which obey natural laws, there also exists \emph{a priori} constraints on those samples.
These constrains can be used to regularise neural networks, vastly reducing the search space for physically possible solutions.

\parencite{liImplicitStochasticGradient2023} replace the ubiquitous stochastic gradient descent optimiser \emph{Adam} with an implicit stochastic gradient descent (ISGD) method:
$$\theta_{n+1} = \theta_{n} - \alpha \cdot \nabla L (\theta_{n+1})$$


\parencite{sethiHardEnforcementPhysicsinformed2023} demonstrate that this regularisation does not have to be implemented as a loss function, but instead can be a \emph{hard} constraint embedded within the network.
This hard constraint takes the form of a function, which satisfies the \emph{a priori} laws, applied to the output of the network prior PDE and regularisation.

Potential fields in an airborne exploration context are constrained by a number of physical laws.
From \parencite{blakelyPotentialTheoryGravity1996}, potential fields satisfy \emph{Laplace's equation},
$$\nabla^2 \phi = 0$$
This indicates the function lacks curvature, in three dimensions $\phi(x,y,z)$:
$$\frac{\partial^2\phi}{\partial x^2} + \frac{\partial^2\phi}{\partial y^2} + \frac{\partial^2\phi}{\partial z^2} = 0$$

\subsection{Geophysical Sampling and Continuation}
\label{sec:geo_sampling}
Potential fields in geophysics are vector fields, but are typically sampled as a scalar in one direction.
A scalar field can be parameterised with a single dimension at each spatial coordinate, while a vector field requires three orthogonal components at each coordinate \parencite{blakelyPotentialTheoryGravity1996}.

$F = \nabla \phi$ for the gravity potential, $F = - \nabla \phi$ for the Magnetic potential.

$\phi(x,y,z) = constant$ is an equipotential surface. If Laplaces equation: $\nabla^2 \phi = 0$ is satisfied, it is harmonic, and has continuous single valued first derivatives, and has second derivatives.


\section{Method}
\subsection{Notation}
$\mathbb{R}^n$ Real n-dimensional vectors, named using lower-case letters, e.g. \boldmath{x}.

$\mathbb{R}^{m\times{}n}$ $m*n$ dimensional matrices, named using upper-case letters, e.g. \boldmath{A}.

\subsection{Differentiation}
A Numerical Differentiation approach to gradient calculation here would be to evaluate $\frac{\partial{U}}{\partial{(x, y, z)}}$ at a set of randomly sampled coordinates $(x, y, z)_1 ... (x,y,z)_n$.
However, using Automatic Differentiation within Pytorch, the continuous implicit represenation can be differentiated to provide gradients without being constrained by numerical precision or quantisation.
These are also unstable, recognised in the geophysical community by divergence and noise in predictions depending on numerical methods.



\subsection{SIREN}
\begin{enumerate}
    \item SIREN (or WIRE, if it improves)
    \item Regularisation loss
    \item Parameter opt
\end{enumerate}

\subsection{Training Particulars}
Performed on an Intel i5-12400 CPU with 32 GB RAM, and an Nvidia 3060 12 GB GPU.

ADAM, One Cycle LR, 1000 iter/epochs.

MSE loss.

\subsection{Data}
GADDS Wolfe Creek impact survey netCDF

Noddyverse

tba

\section{Results}
\subsection{Implicit gridding}
\begin{enumerate}
    \item Best attempt at recreating grid as gridded by GA.
    \item Demonstration of the range of altitude slices
\end{enumerate}

\subsection{Spatial gradients and filters}
\begin{enumerate}
    \item Horizontal Gradients
    \item Vertical Gradients
    \item Combinations and filters
\end{enumerate}

\section{Discussion}
\begin{enumerate}
    \item Limitations: Capacity of Network
    \item RAM usage (if I can't do batched C-MLP training)
    \item Accuracy of learnt x,y and z gradients
    \item Forward looking at the field of INR
\end{enumerate}

Implicit neural representation is a rapidly evolving field of research with many advances occuring within each year, borrowing contemporary deep learning improvements from related fields.
In addition to constant work on improving the high-frequency learning capacity, the inclusion of additional dimensions or

\section{Conclusions}

\section{Acknowledgements}
This project is funded by a Rio Tinto Iron Ore PhD scholarship.

\printbibliography{}

\end{document}